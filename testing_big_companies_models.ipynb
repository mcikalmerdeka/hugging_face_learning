{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting With Famous Open Models\n",
    "\n",
    "## A. General Purpose Models  \n",
    "- **deepseek-ai/DeepSeek-R1-0528** - Deepseek new (May 2025) text generation model\n",
    "- **Llama-2-7B-Chat-hf** - Meta's conversational model\n",
    "- **Mistral-7B-Instruct-v0.2** - Mistral AI's efficient instruction model\n",
    "- **Qwen2-7B-Instruct** - Alibaba's multilingual model\n",
    "- **Phi-3-mini-4k-instruct** - Microsoft's compact but capable model\n",
    "\n",
    "## B. Code Generation Models\n",
    "- **DeepSeek-Coder-V2-Lite-Instruct** - Latest DeepSeek coding model, efficient and capable\n",
    "- **CodeLlama-7B-Instruct-hf** - Meta's code-focused Llama variant\n",
    "- **WizardCoder-15B-V1.0** - Microsoft's enhanced coding model\n",
    "- **StarCoder2-15B** - BigCode's latest generation model\n",
    "\n",
    "## C. Specialized Models\n",
    "- **deepseek-math-7b-instruct** - DeepSeek's math reasoning specialist\n",
    "- **CodeT5p-770M-Python** - Salesforce's Python-focused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initializa text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Hello, how are you? am I using the right model of deepseek R1 1.5B?\"\n",
    "text = text_generator(prompt)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: HuggingFace models download automatically on first use\n",
    "\n",
    "- Can be several GB per model (e.g., 7B models ~13GB, 1.5B models ~3GB)\n",
    "- Downloads to ~/.cache/huggingface/transformers/ by default\n",
    "- Use ollama for local models to avoid repeated downloads\n",
    "- Check disk space before running new models\n",
    "\n",
    "To use them we can just download the model first using ollama with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n",
      "</think>\n",
      "\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Use local Ollama model instead of downloading from HuggingFace\n",
    "# Make sure you have the model pulled: ollama pull deepseek-r1:1.5b\n",
    "response = ollama.chat(\n",
    "    model='deepseek-r1:1.5b',  # or whatever name you used when pulling\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Hello, how are you? am I using the right model of deepseek R1 1.5B?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
